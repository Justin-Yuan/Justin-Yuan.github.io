<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> publications | Justin (Zhaocong) Yuan </title> <meta name="author" content="Justin (Zhaocong) Yuan"> <meta name="description" content="(*) denotes equal contribution"> <meta name="keywords" content="Justin Yuan, machine learning, AI, artificial intelligence"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/himmel_circle.png?adef9dbdc5aa4238d0ff52f77571f609"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://justin-yuan.github.io/publications/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Justin (Zhaocong)</span> Yuan </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item active"> <a class="nav-link" href="/publications/">publications <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link"> <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">publications</h1> <p class="post-description">(*) denotes equal contribution</p> </header> <article> <div class="publications"> <h2 class="bibliography">2025</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">NeurIPS</abbr> </div> <div id="ramakrishnan2025omnidraft" class="col-sm-8"> <div class="title">OmniDraft: A Cross-vocabulary, Online Adaptive Drafter for On-device Speculative Decoding</div> <div class="author"> <a href="https://www.linkedin.com/in/ramchalamkr/" rel="external nofollow noopener" target="_blank">Ramchalam Kinattinkara Ramakrishnan</a>, <em>Zhaocong Yuan</em>, <a href="https://www.linkedin.com/in/shaojie-zhuo-1b535121/" rel="external nofollow noopener" target="_blank">Shaojie Zhuo</a>, <a href="https://www.linkedin.com/in/chen-feng-84603638/" rel="external nofollow noopener" target="_blank">Chen Feng</a>, <a href="https://www.linkedin.com/in/yicheng-lin-3b579b20/" rel="external nofollow noopener" target="_blank">Yicheng Lin</a>, <a href="https://www.linkedin.com/in/jokaysu/" rel="external nofollow noopener" target="_blank">Chenzheng Su</a>, and <a href="https://www.linkedin.com/in/parker-xiaopeng-zhang-56433b6/" rel="external nofollow noopener" target="_blank">Xiaopeng Zhang</a> </div> <div class="periodical"> <em>arXiv preprint arXiv:2507.02659</em>, 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a> <a href="https://arxiv.org/abs/2507.02659" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>Speculative decoding generally dictates having a small, efficient draft model that is either pretrained or distilled offline to a particular target model series, for instance, Llama or Qwen models. However, within online deployment settings, there are two major challenges: 1) usage of a target model that is incompatible with the draft model; 2) expectation of latency improvements over usage and time. In this work, we propose OmniDraft, a unified framework that enables a single draft model to operate with any target model and adapt dynamically to user data. We introduce an online n-gram cache with hybrid distillation fine-tuning to address the cross-vocabulary mismatch across draft and target models; and further improve decoding speed by leveraging adaptive drafting techniques. OmniDraft is particularly suitable for on-device LLM applications where model cost, efficiency and user customization are the major points of contention. This further highlights the need to tackle the above challenges and motivates the \textit“one drafter for all” paradigm. We showcase the proficiency of the OmniDraft framework by performing online learning on math reasoning, coding and text generation tasks. Notably, OmniDraft enables a single Llama-68M model to pair with various target models including Vicuna-7B, Qwen2-7B and Llama3-8B models for speculative decoding; and additionally provides up to 1.5-2x speedup.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">arXiv</abbr> </div> <div id="feng2025edge" class="col-sm-8"> <div class="title">Edge-ASR: Towards Low-Bit Quantization of Automatic Speech Recognition Models</div> <div class="author"> <a href="https://www.linkedin.com/in/chen-feng-84603638/" rel="external nofollow noopener" target="_blank">Chen Feng</a>, <a href="https://www.linkedin.com/in/yicheng-lin-3b579b20/" rel="external nofollow noopener" target="_blank">Yicheng Lin</a>, <a href="https://www.linkedin.com/in/shaojie-zhuo-1b535121/" rel="external nofollow noopener" target="_blank">Shaojie Zhuo</a>, <a href="https://www.linkedin.com/in/jokaysu/" rel="external nofollow noopener" target="_blank">Chenzheng Su</a>, <a href="https://www.linkedin.com/in/ramchalamkr/" rel="external nofollow noopener" target="_blank">Ramchalam Kinattinkara Ramakrishnan</a>, <em>Zhaocong Yuan</em>, and <a href="https://www.linkedin.com/in/parker-xiaopeng-zhang-56433b6/" rel="external nofollow noopener" target="_blank">Xiaopeng Zhang</a> </div> <div class="periodical"> <em>arXiv preprint arXiv:2507.07877</em>, 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a> <a href="https://arxiv.org/abs/2507.07877" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>Recent advances in Automatic Speech Recognition (ASR) have demonstrated remarkable accuracy and robustness in diverse audio applications, such as live transcription and voice command processing. However, deploying these models on resource-constrained edge devices (e.g., IoT device, wearables) still presents substantial challenges due to strict limits on memory, compute and power. Quantization, particularly Post-Training Quantization (PTQ), offers an effective way to reduce model size and inference cost without retraining. Despite its importance, the performance implications of various advanced quantization methods and bit-width configurations on ASR models remain unclear. In this work, we present a comprehensive benchmark of eight state-of-the-art (SOTA) PTQ methods applied to two leading edge-ASR model families, Whisper and Moonshine. We systematically evaluate model performances (i.e., accuracy, memory I/O and bit operations) across seven diverse datasets from the open ASR leader-board, analyzing the impact of quantization and various configurations on both weights and activations. Built on an extension of the LLM compression toolkit, our framework integrates edge-ASR models, diverse advanced quantization algorithms, a unified calibration and evaluation data pipeline, with detailed analysis tools. Our results characterize the trade-offs between efficiency and accuracy, demonstrating that even 3-bit quantization can succeed on high capacity models when using advanced PTQ techniques. These findings provide valuable insights for optimizing ASR models on low-power, always-on edge devices.</p> </div> </div> </div> </li> </ol> <h2 class="bibliography">2024</h2> <ol class="bibliography"><li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">NeurIPS</abbr> </div> <div id="feng2024stepping" class="col-sm-8"> <div class="title">Stepping forward on the last mile</div> <div class="author"> <a href="https://www.linkedin.com/in/chen-feng-84603638/" rel="external nofollow noopener" target="_blank">Chen Feng</a> , Jay Zhuo , Parker Zhang, Ramchalam Kinattinkara Ramakrishnan, <em>Zhaocong Yuan</em>, and Andrew Zou Li </div> <div class="periodical"> <em>Advances in Neural Information Processing Systems</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a> <a href="https://arxiv.org/abs/2411.04036" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>Continuously adapting pre-trained models to local data on resource constrained edge devices is the \emphlast mile for model deployment. However, as models increase in size and depth, backpropagation requires a large amount of memory, which becomes prohibitive for edge devices. In addition, most existing low power neural processing engines (e.g., NPUs, DSPs, MCUs, etc.) are designed as fixed-point inference accelerators, without training capabilities. Forward gradients, solely based on directional derivatives computed from two forward calls, have been recently used for model training, with substantial savings in computation and memory. However, the performance of quantized training with fixed-point forward gradients remains unclear. In this paper, we investigate the feasibility of on-device training using fixed-point forward gradients, by conducting comprehensive experiments across a variety of deep learning benchmark tasks in both vision and audio domains. We propose a series of algorithm enhancements that further reduce the memory footprint, and the accuracy gap compared to backpropagation. An empirical study on how training with forward gradients navigates in the loss landscape is further explored. Our results demonstrate that on the last mile of model customization on edge devices, training with fixed-point forward gradients is a feasible and practical approach.</p> </div> </div> </div> </li></ol> <h2 class="bibliography">2022</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">NeurIPS Workshop</abbr> </div> <div id="glossop2022characterising" class="col-sm-8"> <div class="title">Characterising the Robustness of Reinforcement Learning for Continuous Control using Disturbance Injection</div> <div class="author"> <a href="https://www.linkedin.com/in/amritkrishnan/?originalSubdomain=ca" rel="external nofollow noopener" target="_blank">Catherine Glossop</a>, <a href="https://jacopopanerati.github.io/" rel="external nofollow noopener" target="_blank">Jacopo Panerati</a>, <a href="https://www.linkedin.com/in/amritkrishnan/?originalSubdomain=ca" rel="external nofollow noopener" target="_blank">Amrit Krishnan</a>, <em>Zhaocong Yuan</em>, and <a href="https://www.dynsyslab.org/prof-angela-schoellig/" rel="external nofollow noopener" target="_blank">Angela P. Schoellig</a> </div> <div class="periodical"> <em>In NeurIPS 2022 Workshop on Distribution Shifts: Connecting Methods and Applications</em>, 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a> <a href="https://openreview.net/forum?id=dJPzobZtpZ" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://vectorinstitute.ai/vector-ai-engineering-blog-benchmarking-robustness-of-reinforcement-learning-approaches-using-safe-control-gym/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Blog</a> </div> <div class="abstract hidden"> <p>In this study, we leverage the deliberate and systematic fault-injection capabilities of an open-source benchmark suite to perform a series of experiments on state-of-the-art deep and robust reinforcement learning algorithms. We aim to benchmark robustness in the context of continuous action spaces – crucial for deployment in robot control. We find that robustness is more prominent for action disturbances than it is for disturbances to observations and dynamics. We also observe that state-of-the-art approaches that are not explicitly designed to improve robustness perform at a level comparable to that achieved by those that are. Our study and results are intended to provide insight into the current state of safe and robust reinforcement learning and a foundation for the advancement of the field, in particular, for deployment in robotic systems.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">Thesis</abbr> </div> <div id="yuan2022benchmarking" class="col-sm-8"> <div class="title">Benchmarking Reinforcement Learning for Safe Robotics: Constraints, Robustness and Transfer</div> <div class="author"> <em>Zhaocong Yuan</em> </div> <div class="periodical"> <em>University of Toronto</em>, 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a> <a href="https://tspace.library.utoronto.ca/handle/1807/125223" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/Justin-Yuan/dsl__projects__benchmark/tree/benchmark" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>Safe learning in robotics aims to deploy robots in real life with complex tasks and safety requirements. To push the agenda of safe learning, a crucial step is establishing common benchmarks that facilitate both reliable evaluations and research development. In this work, we contribute towards this goal by surveying the safe learning literature and proposing a versatile safe learning benchmark suite, safe-control-gym. The benchmark implements a variety of safe learning algorithms spanning control to reinforcement learning, it also implements critical features to support safety-relevant evaluations and algorithm development. With safe-control-gym, we conduct careful benchmarking on model-free reinforcement learning methods with respect to three metrics of safety: constraint satisfaction, robustness, and transfer performance. We envision safe-control-gym to provide a framework that brings various research together, and most importantly to accelerate the progress of safe learning in robotics.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">IROS</abbr> </div> <div id="yuan2022safe" class="col-sm-8"> <div class="title">safe-Control-Gym: A Unified Benchmark Suite for Safe Learning-Based Control and Reinforcement Learning in Robotics</div> <div class="author"> <em>Zhaocong Yuan</em>, <a href="https://adamhall.github.io/" rel="external nofollow noopener" target="_blank">Adam W Hall</a>, <a href="https://siqizhou.com/" rel="external nofollow noopener" target="_blank">Siqi Zhou</a>, <a href="https://www.lukasbrunke.com/" rel="external nofollow noopener" target="_blank">Lukas Brunke</a>, <a href="https://roboralab.com/team.html" rel="external nofollow noopener" target="_blank">Melissa Greeff</a>, <a href="https://jacopopanerati.github.io/" rel="external nofollow noopener" target="_blank">Jacopo Panerati</a>, and <a href="https://www.dynsyslab.org/prof-angela-schoellig/" rel="external nofollow noopener" target="_blank">Angela P Schoellig</a> </div> <div class="periodical"> <em>IEEE Robotics and Automation Letters</em>, 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a> <a href="https://arxiv.org/abs/2109.06325" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/utiasDSL/safe-control-gym/tree/main" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>In recent years, both reinforcement learning and learning-based control—as well as the study of their safety, which is crucial for deployment in real-world robots—have gained significant traction. However, to adequately gauge the progress and applicability of new results, we need the tools to equitably compare the approaches proposed by the controls and reinforcement learning communities. Here, we propose a new open-source benchmark suite, called safe-control-gym, supporting both model-based and databased control techniques. We provide implementations for three dynamic systems—the cart-pole, the 1D, and 2D quadrotor— and two control tasks—stabilization and trajectory tracking. We propose to extend OpenAI’s Gym API—the de facto standard in reinforcement learning research—with (i) the ability to specify (and query) symbolic dynamics and (ii) constraints, and (iii) (repeatably) inject simulated disturbances in the control inputs, state measurements, and inertial properties. To demonstrate our proposal and in an attempt to bring research communities closer together, we show how to use safe-control-gym to quantitatively compare the control performance, data efficiency, and safety of multiple approaches from the fields of traditional control, learning-based control, and reinforcement learning.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">Annual Reviews</abbr> </div> <div id="brunke2022safe" class="col-sm-8"> <div class="title">Safe learning in robotics: From learning-based control to safe reinforcement learning</div> <div class="author"> <a href="https://www.lukasbrunke.com/" rel="external nofollow noopener" target="_blank">Lukas Brunke</a>, <a href="https://roboralab.com/team.html" rel="external nofollow noopener" target="_blank">Melissa Greeff</a>, <a href="https://adamhall.github.io/" rel="external nofollow noopener" target="_blank">Adam W Hall</a>, <em>Zhaocong Yuan</em>, <a href="https://siqizhou.com/" rel="external nofollow noopener" target="_blank">Siqi Zhou</a>, <a href="https://jacopopanerati.github.io/" rel="external nofollow noopener" target="_blank">Jacopo Panerati</a>, and <a href="https://www.dynsyslab.org/prof-angela-schoellig/" rel="external nofollow noopener" target="_blank">Angela P Schoellig</a> </div> <div class="periodical"> <em>Annual Review of Control, Robotics, and Autonomous Systems</em>, 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a> <a href="https://www.annualreviews.org/content/journals/10.1146/annurev-control-042920-020211" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/utiasDSL/safe-control-gym/tree/ar" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>The last half decade has seen a steep rise in the number of contributions on safe learning methods for real-world robotic deployments from both the control and reinforcement learning communities. This article provides a concise but holistic review of the recent advances made in using machine learning to achieve safe decision-making under uncertainties, with a focus on unifying the language and frameworks used in control theory and reinforcement learning research. It includes learning-based control approaches that safely improve performance by learning the uncertain dynamics, reinforcement learning approaches that encourage safety or robustness, and methods that can formally certify the safety of a learned control policy. As data- and learning-based robot control methods continue to gain traction, researchers must understand when and how to best leverage them in real-world scenarios where safety is imperative, such as when operating in close proximityto humans. We highlight some of the open challenges that will drive the field of robot learning in the coming years, and emphasize the need for realistic physics-based benchmarks to facilitate fair comparisons between control and reinforcement learning approaches.</p> </div> </div> </div> </li> </ol> <h2 class="bibliography">2020</h2> <ol class="bibliography"><li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">Thesis</abbr> </div> <div id="yuan2020multi" class="col-sm-8"> <div class="title">Emergent Communication Behaviors in Multi-Agent Systems</div> <div class="author"> <em>Zhaocong Yuan</em> </div> <div class="periodical"> <em>University of Toronto</em>, 2020 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a> <a href="https://tspace.library.utoronto.ca/handle/1807/125223" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/Justin-Yuan/learn-to-interact" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>Multi-agent systems are prevalent in real life, powering the majority of people’s everyday activities. Multi-agent Reinforcement Learning is the study of these systems by means of machine learning, it has gained much attention in successfully applying to games, robotics and language understanding. This work concerns the emeregent behavior aspeect of MARL, speecifically emergent communications. We extend the MADDPG framework and experiment on a variety of multi-agent tasks from the eOpenAI Multi-agent Particle Environments, which span different behavior modes including cooperationo, competitiono, communication and mixture of those. Besides demonstrating the emergent behavior learnt from each task, we further provide analysis and insights regarding the MARL model design and training. At the end, we outline several possible lines for future work, hoping to inspire more multi-agent related research and advance the field of MARL.</p> </div> </div> </div> </li></ol> <h2 class="bibliography">2019</h2> <ol class="bibliography"><li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">ICCV</abbr> </div> <div id="kar2019meta" class="col-sm-8"> <div class="title">Meta-sim: Learning to generate synthetic datasets</div> <div class="author"> <a href="https://amlankar.github.io/" rel="external nofollow noopener" target="_blank">Amlan Kar</a>, <a href="https://aayushp.github.io/" rel="external nofollow noopener" target="_blank">Aayush Prakash</a>, <a href="https://mingyuliu.net/" rel="external nofollow noopener" target="_blank">Ming-Yu Liu</a>, <a href="https://www.linkedin.com/in/eric-cameracci-b926505a/?originalSubdomain=ca" rel="external nofollow noopener" target="_blank">Eric Cameracci</a>, <em>Justin Yuan</em>, <a href="https://www.linkedin.com/in/mrusiniak/?originalSubdomain=ch" rel="external nofollow noopener" target="_blank">Matt Rusiniak</a>, <a href="http://www.cs.toronto.edu/~davidj/" rel="external nofollow noopener" target="_blank">David Acuna</a>, <a href="https://groups.csail.mit.edu/vision/torralbalab/" rel="external nofollow noopener" target="_blank">Antonio Torralba</a>, and <a href="https://www.cs.utoronto.ca/~fidler/" rel="external nofollow noopener" target="_blank">Sanja Fidler</a> </div> <div class="periodical"> <em>In Proceedings of the IEEE/CVF International Conference on Computer Vision</em>, 2019 </div> <div class="periodical"> </div> <div class="links"> <a class="award btn btn-sm z-depth-0" role="button">Awarded</a> <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a> <a href="https://arxiv.org/abs/1904.11621" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/nv-tlabs/meta-sim" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="https://research.nvidia.com/labs/toronto-ai/meta-sim/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> </div> <div class="award hidden d-print-inline"> <p></p> <p>Oral</p> </div> <div class="abstract hidden"> <p>Training models to high-end performance requires availability of large labeled datasets, which are expensive to get. The goal of our work is to automatically synthesize labeled datasets that are relevant for a downstream task. We propose Meta-Sim, which learns a generative model of synthetic scenes, and obtain images as well as its corresponding ground-truth via a graphics engine. We parametrize our dataset generator with a neural network, which learns to modify attributes of scene graphs obtained from probabilistic scene grammars, so as to minimize the distribution gap between its rendered outputs and target data. If the real dataset comes with a small labeled validation set, we additionally aim to optimize a meta-objective, i.e. downstream task performance. Experiments show that the proposed method can greatly improve content generation quality over a human-engineered probabilistic scene grammar, both qualitatively and quantitatively as measured by performance on a downstream task.</p> </div> </div> </div> </li></ol> </div> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Justin (Zhaocong) Yuan. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let searchTheme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===searchTheme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-about",title:"about",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-blog",title:"blog",description:"",section:"Navigation",handler:()=>{window.location.href="/blog/"}},{id:"nav-publications",title:"publications",description:"(&#x2A) denotes equal contribution",section:"Navigation",handler:()=>{window.location.href="/publications/"}},{id:"nav-cv",title:"cv",description:"",section:"Navigation",handler:()=>{window.location.href="/cv/"}},{id:"post-vector-quantization-a-quick-dive",title:"Vector Quantization - A Quick Dive",description:"vector_quantization, generative_models, machine_learning",section:"Posts",handler:()=>{window.location.href="/blog/2024/vector-quantization/"}},{id:"news-intern-at-apple-seattle-on-the-siri-nlu-team",title:"Intern at Apple, Seattle, on the Siri NLU team",description:"",section:"News"},{id:"news-intern-at-nvidia-toronto-deep-learning-research-on-sim-to-real-transfer-and-self-driving",title:"Intern at Nvidia, Toronto, deep learning research on Sim-to-Real transfer and self-driving",description:"",section:"News"},{id:"news-graduate-from-university-of-toronto-basc-engineering-science",title:"Graduate from University of Toronto, BASc, Engineering Science",description:"",section:"News"},{id:"news-vector-institute-industry-workshop-guest-speaker",title:"Vector Institute Industry Workshop (guest speaker).",description:"",section:"News"},{id:"news-icra-2022-workshop-on-releasing-robots-into-the-wild-simulations-benchmarks-and-deployment-organizer-website-videos",title:"ICRA 2022 Workshop on Releasing Robots into the Wild: Simulations, Benchmarks, and Deployment...",description:"",section:"News"},{id:"news-iros-2022-presentation-of-our-safe-control-gym-paper-link",title:"IROS 2022 presentation of our safe-control-gym paper. [link]",description:"",section:"News"},{id:"news-graduate-from-university-of-toronto-masc",title:"Graduate from University of Toronto, MASc",description:"",section:"News"},{id:"news-join-qualcomm-as-a-machine-learning-research-engineer-on-the-embedded-ai-team",title:"Join Qualcomm as a Machine Learning Research Engineer on the Embedded AI team....",description:"",section:"News"},{id:"news-paper-stepping-forward-on-the-last-mile-accepted-at-neurips-2024-link-sparkles-smile",title:'Paper \u201cStepping Forward on the Last Mile\u201d accepted at NeurIPS 2024. [link] <img class="emoji" title=":sparkles:" alt=":sparkles:" src="https://github.githubassets.com/images/icons/emoji/unicode/2728.png" height="20" width="20">...',description:"",section:"News"},{id:"news-paper-omnidraft-a-cross-vocabulary-online-adaptive-drafter-for-on-device-speculative-decoding-accepted-at-neurips-2025-link-sparkles-sparkles-sparkles",title:"Paper \u201cOmniDraft: A Cross-vocabulary, Online Adaptive Drafter for On-device Speculative Decoding \u201c accepted...",description:"",section:"News"},{id:"projects-project-1",title:"project 1",description:"with background image",section:"Projects",handler:()=>{window.location.href="/projects/1_project/"}},{id:"projects-project-2",title:"project 2",description:"a project with a background image and giscus comments",section:"Projects",handler:()=>{window.location.href="/projects/2_project/"}},{id:"projects-project-3-with-very-long-name",title:"project 3 with very long name",description:"a project that redirects to another website",section:"Projects",handler:()=>{window.location.href="/projects/3_project/"}},{id:"projects-project-4",title:"project 4",description:"another without an image",section:"Projects",handler:()=>{window.location.href="/projects/4_project/"}},{id:"projects-project-5",title:"project 5",description:"a project with a background image",section:"Projects",handler:()=>{window.location.href="/projects/5_project/"}},{id:"projects-project-6",title:"project 6",description:"a project with no image",section:"Projects",handler:()=>{window.location.href="/projects/6_project/"}},{id:"projects-project-7",title:"project 7",description:"with background image",section:"Projects",handler:()=>{window.location.href="/projects/7_project/"}},{id:"projects-project-8",title:"project 8",description:"an other project with a background image and giscus comments",section:"Projects",handler:()=>{window.location.href="/projects/8_project/"}},{id:"projects-project-9",title:"project 9",description:"another project with an image \ud83c\udf89",section:"Projects",handler:()=>{window.location.href="/projects/9_project/"}},{id:"socials-email",title:"Send email",section:"Socials",handler:()=>{window.open("mailto:%6A%75%73%74%69%6E.%7A%63%79%75%61%6E@%67%6D%61%69%6C.%63%6F%6D","_blank")}},{id:"socials-google-scholar",title:"Google Scholar",section:"Socials",handler:()=>{window.open("https://scholar.google.com/citations?user=4mfGXkAAAAJ","_blank")}},{id:"socials-github",title:"GitHub",section:"Socials",handler:()=>{window.open("https://github.com/Justin-Yuan","_blank")}},{id:"socials-linkedin",title:"LinkedIn",section:"Socials",handler:()=>{window.open("https://www.linkedin.com/in/justin-yuan","_blank")}},{id:"socials-x",title:"X",description:"Twitter",section:"Socials",handler:()=>{window.open("https://twitter.com/justin_zcyuan","_blank")}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}];</script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>